{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c63450b",
   "metadata": {},
   "source": [
    "# Customer Churn Prediction for Beta Bank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8503be",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Customer churn, also known as customer attrition, refers to the phenomenon of customers leaving a company. It's a critical metric in many businesses, as it's often less expensive to retain existing customers than to attract new ones. Moreover, reducing customer churn is directly related to increasing customer lifetime value.\n",
    "\n",
    "In this project, we are working with Beta Bank. The bank has been noticing an increase in customer churn rates. The bankers figured out it’s cheaper to save the existing customers rather than to attract new ones.\n",
    "\n",
    "We have been provided with a dataset containing data on clients’ past behavior and termination of contracts with the bank. Our task is to predict whether a customer will leave the bank soon. The goal is to create a machine learning model to predict customer churn so that the bank can proactively address the issue and improve customer retention.\n",
    "\n",
    "The project involves the following steps:\n",
    "\n",
    "1. **Data Preparation:** Download and prepare the data. Explain the procedure.\n",
    "2. **Examine the balance of classes:** Check if our target variable 'Exited' is balanced or imbalanced. A class imbalance could affect the performance of our model.\n",
    "3. **Train the model without taking into account the imbalance:** We will first train our model without considering the possible class imbalance. This will serve as a baseline for comparison.\n",
    "4. **Improve the quality of the model:** If there is a class imbalance, we will use at least two approaches to fix it and improve the quality of our model. We will use the training set to pick the best parameters.\n",
    "5. **Perform the final testing:** Finally, we will evaluate the performance of our model on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2869f311",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "In this step, we will download and prepare the data for analysis. The data preparation process involves the following steps:\n",
    "\n",
    "1. **Downloading the data:** We will download the data from the provided URL.\n",
    "2. **Loading the data:** We will load the data into a pandas DataFrame, which is a 2-dimensional labeled data structure with columns of potentially different types. DataFrames are generally the most commonly used pandas object.\n",
    "3. **Inspecting the data:** We will inspect the data to understand its structure and the types of data it contains. This includes checking the number of rows and columns, the types of variables, and the number of missing values.\n",
    "4. **Handling missing values:** If there are any missing values in the data, we will decide how to handle them. This could involve removing rows or columns with missing values, or filling in the missing values with a specific value.\n",
    "5. **Encoding categorical variables:** If there are any categorical variables in the data, we will encode them. Machine learning models require input to be in numerical format, so we need to convert categorical variables into a suitable numerical format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9b92709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8.0</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2.0</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0     2.0       0.00              1          1               1   \n",
       "1     1.0   83807.86              1          0               1   \n",
       "2     8.0  159660.80              3          1               0   \n",
       "3     1.0       0.00              2          0               0   \n",
       "4     2.0  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('Churn.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6b5075",
   "metadata": {},
   "source": [
    "## Initial Data Analysis\n",
    "In this step, we will inspect the data to understand its structure and the types of data it contains. This includes checking the number of rows and columns, the types of variables, and the number of missing values.\n",
    "\n",
    "Understanding the structure of the data will help us determine the necessary data preprocessing steps. For example, if there are missing values, we will need to decide how to handle them. If there are categorical variables, we will need to encode them.\n",
    "\n",
    "Let's start by checking the number of rows and columns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d53ac5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has 10000 rows and 14 columns.\n"
     ]
    }
   ],
   "source": [
    "# Checking the number of rows and columns in the data\n",
    "print(f'The data has {data.shape[0]} rows and {data.shape[1]} columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "249a74af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RowNumber            int64\n",
       "CustomerId           int64\n",
       "Surname             object\n",
       "CreditScore          int64\n",
       "Geography           object\n",
       "Gender              object\n",
       "Age                  int64\n",
       "Tenure             float64\n",
       "Balance            float64\n",
       "NumOfProducts        int64\n",
       "HasCrCard            int64\n",
       "IsActiveMember       int64\n",
       "EstimatedSalary    float64\n",
       "Exited               int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the data types of each column\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb092666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RowNumber            0\n",
       "CustomerId           0\n",
       "Surname              0\n",
       "CreditScore          0\n",
       "Geography            0\n",
       "Gender               0\n",
       "Age                  0\n",
       "Tenure             909\n",
       "Balance              0\n",
       "NumOfProducts        0\n",
       "HasCrCard            0\n",
       "IsActiveMember       0\n",
       "EstimatedSalary      0\n",
       "Exited               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for missing values in each column\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99fceaa",
   "metadata": {},
   "source": [
    "## Handling Missing Values\n",
    "In the data inspection step, we found that the 'Tenure' column has 909 missing values. Missing values can affect the performance of a machine learning model, and it's important to handle them before training the model.\n",
    "\n",
    "There are several strategies to handle missing values, including removing rows with missing values and imputing missing values. Removing rows with missing values is the simplest strategy, but it can lead to loss of information if many rows have missing values. Imputing missing values involves filling in the missing values with a specific value. The value could be a central tendency measure like the mean or median (for numerical variables) or the mode (for categorical variables). Alternatively, it could be a value estimated by a machine learning model.\n",
    "\n",
    "In our case, since 'Tenure' is a numerical variable, we will impute the missing values with the median 'Tenure'. The median is a robust measure of central tendency that is not affected by outliers, making it a suitable choice for imputation.\n",
    "\n",
    "We will also drop the Surname, RowNumber, and CustomerId columns as they are not relevant for our analysis. These columns are unique identifiers and do not contribute to a customer's churn probability.\n",
    "\n",
    "Let's proceed with imputing the missing values in the 'Tenure' column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6e5bfa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CreditScore        0\n",
       "Geography          0\n",
       "Gender             0\n",
       "Age                0\n",
       "Tenure             0\n",
       "Balance            0\n",
       "NumOfProducts      0\n",
       "HasCrCard          0\n",
       "IsActiveMember     0\n",
       "EstimatedSalary    0\n",
       "Exited             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the median 'Tenure'\n",
    "median_tenure = data['Tenure'].median()\n",
    "\n",
    "# Imputing the missing values in the 'Tenure' column with the median 'Tenure'\n",
    "data['Tenure'].fillna(median_tenure, inplace=True)\n",
    "\n",
    "# Drop 'Surname', 'RowNumber', and 'CustomerId' columns\n",
    "data = data.drop(['Surname', 'RowNumber', 'CustomerId'], axis=1)\n",
    "\n",
    "# Checking for missing values in each column to verify\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f74750",
   "metadata": {},
   "source": [
    "## Examining the Balance of Classes\n",
    "In this step, we will check if our target variable 'Exited' is balanced or imbalanced. A class imbalance could affect the performance of our model.\n",
    "\n",
    "Class imbalance refers to a situation where the classes in the target variable are not represented equally. For example, in a binary classification problem, if 90% of the samples belong to Class A and only 10% belong to Class B, we have a severe class imbalance.\n",
    "\n",
    "Class imbalance can lead to a misleadingly high accuracy rate. For example, a model that always predicts Class A in the above scenario will be 90% accurate, even though it's not identifying Class B at all. Therefore, it's important to check for class imbalance and take it into account when training the model and evaluating its performance.\n",
    "\n",
    "Let's check the balance of classes in the 'Exited' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1105edc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7963\n",
       "1    2037\n",
       "Name: Exited, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the balance of classes in the 'Exited' column\n",
    "class_counts = data['Exited'].value_counts()\n",
    "class_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38ff4de",
   "metadata": {},
   "source": [
    "The 'Exited' column, which is our target variable, is imbalanced. There are 7,963 customers who have not exited the bank (represented by 0) and 2,037 customers who have exited the bank (represented by 1). From the output, we can see that about 79.63% of the customers have not exited (class 0), while about 20.37% of the customers have exited (class 1). \n",
    "\n",
    "This imbalance in the classes can lead to a bias in the model towards predicting the majority class. Therefore, we need to take this into account when training our model.\n",
    "\n",
    "But before we handle the class imbalance, let's first train a model without taking into account the imbalance and see how it performs. This will give us a baseline performance that we can compare against after we handle the class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa9e5ce",
   "metadata": {},
   "source": [
    "## Encoding Categorical Variables\n",
    "Machine learning models require input to be in numerical format. However, our data contains categorical variables, specifically the 'Geography', and 'Gender' columns. We need to convert these categorical variables into a suitable numerical format.\n",
    "\n",
    "There are several strategies to encode categorical variables, including label encoding and one-hot encoding. Label encoding involves assigning each unique category in a categorical variable with an integer. One-hot encoding involves creating a new binary column for each unique category in a categorical variable.\n",
    "\n",
    "First, let's encode the categorical variables. The 'Geography' and 'Gender' columns are categorical and need to be encoded. We will use one-hot encoding for 'Geography' since it is a nominal variable (i.e., there is no order in the categories). For 'Gender', we will use label encoding since it is a binary variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d3247c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>619.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>608.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>502.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>699.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>850.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1    2      3    4     5    6          7    8    9    10         11  \\\n",
       "0  1.0  0.0  0.0  619.0  0.0  42.0  2.0       0.00  1.0  1.0  1.0  101348.88   \n",
       "1  0.0  0.0  1.0  608.0  0.0  41.0  1.0   83807.86  1.0  0.0  1.0  112542.58   \n",
       "2  1.0  0.0  0.0  502.0  0.0  42.0  8.0  159660.80  3.0  1.0  0.0  113931.57   \n",
       "3  1.0  0.0  0.0  699.0  0.0  39.0  1.0       0.00  2.0  0.0  0.0   93826.63   \n",
       "4  0.0  0.0  1.0  850.0  0.0  43.0  2.0  125510.82  1.0  1.0  1.0   79084.10   \n",
       "\n",
       "    12  \n",
       "0  1.0  \n",
       "1  0.0  \n",
       "2  1.0  \n",
       "3  0.0  \n",
       "4  0.0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Label encode 'Gender' column\n",
    "le = LabelEncoder()\n",
    "data['Gender'] = le.fit_transform(data['Gender'])\n",
    "\n",
    "# One-hot encode 'Geography' column\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1])], remainder='passthrough')\n",
    "data = pd.DataFrame(ct.fit_transform(data))\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0dc8b540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Geography_France</th>\n",
       "      <th>Geography_Germany</th>\n",
       "      <th>Geography_Spain</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>619.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>608.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>502.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>699.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>850.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Geography_France  Geography_Germany  Geography_Spain  CreditScore  Gender  \\\n",
       "0               1.0                0.0              0.0        619.0     0.0   \n",
       "1               0.0                0.0              1.0        608.0     0.0   \n",
       "2               1.0                0.0              0.0        502.0     0.0   \n",
       "3               1.0                0.0              0.0        699.0     0.0   \n",
       "4               0.0                0.0              1.0        850.0     0.0   \n",
       "\n",
       "    Age  Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0  42.0     2.0       0.00            1.0        1.0             1.0   \n",
       "1  41.0     1.0   83807.86            1.0        0.0             1.0   \n",
       "2  42.0     8.0  159660.80            3.0        1.0             0.0   \n",
       "3  39.0     1.0       0.00            2.0        0.0             0.0   \n",
       "4  43.0     2.0  125510.82            1.0        1.0             1.0   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88     1.0  \n",
       "1        112542.58     0.0  \n",
       "2        113931.57     1.0  \n",
       "3         93826.63     0.0  \n",
       "4         79084.10     0.0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign appropriate column names after one-hot encoding\n",
    "data.columns = ['Geography_France', 'Geography_Germany', 'Geography_Spain', 'CreditScore', 'Gender', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary', 'Exited']\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965c1a0f",
   "metadata": {},
   "source": [
    "The 'Geography' and 'Gender' columns have been successfully encoded. The 'Geography' column has been one-hot encoded into three columns (one for each country), and the 'Gender' column has been label encoded (with 'Female' as 0 and 'Male' as 1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6365039",
   "metadata": {},
   "source": [
    "## Training the Model without Taking into Account the Imbalance\n",
    "In this step, we will train a machine learning model without considering the class imbalance in the 'Exited' column. This will serve as a baseline for comparison when we later address the class imbalance and improve the model.\n",
    "\n",
    "Next, let's split the data into features (X) and the target variable (y), and then split these into training and test sets. We will use 80% of the data for training and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9a0c1ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (8000, 12)\n",
      "y_train shape: (8000,)\n",
      "X_test shape: (2000, 12)\n",
      "y_test shape: (2000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate the features and target variable\n",
    "X = data.drop('Exited', axis=1)\n",
    "y = data['Exited']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the training and test sets\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56883c95",
   "metadata": {},
   "source": [
    "The data has been successfully split into training and test sets. The training set contains 8,000 samples and the test set contains 2,000 samples.\n",
    "\n",
    "Next, we will scale the features. Feature scaling is a method used to standardize the range of independent variables or features of data. It is basically scaling all the dimensions to be even, so one independent variable does not dominate others. In our case, we will use StandardScaler from sklearn, which will normalize the features (each column of X, individually) so that each column/feature/variable will have mean = 0 and standard deviation = 1.\n",
    "\n",
    "Note that for cross-validation it's better to apply scaling in each fold separately to avoid potential data leakage. The easiest way to achieve this is using pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "32829e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()), (&#x27;model&#x27;, LogisticRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()), (&#x27;model&#x27;, LogisticRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()), ('model', LogisticRegression())])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a pipeline with a scaler and a logistic regression model\n",
    "pipeline = Pipeline([('scaler', StandardScaler()), ('model', LogisticRegression())])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f615d12",
   "metadata": {},
   "source": [
    "## Training a Baseline Model\n",
    "\n",
    "We will train a logistic regression model without taking into account the imbalance in the classes. This will serve as our baseline model. We will evaluate the model using the F1 score and the AUC-ROC metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "65d775a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.29532710280373836, 0.5809071634753171)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate the F1 score and AUC-ROC score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "f1, roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068602af",
   "metadata": {},
   "source": [
    "The logistic regression model, trained without taking into account the class imbalance, has an F1 score of 0.295 and an AUC-ROC score of 0.58 on the test set. The F1 score is quite low, which is not surprising given the imbalance in the classes. The AUC-ROC score is slightly better, but still not great.\n",
    "\n",
    "Next, let's try to improve the quality of the model by handling the class imbalance. We will use two approaches to handle the class imbalance: oversampling the minority class and undersampling the majority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207ce77f",
   "metadata": {},
   "source": [
    "## Handling Class Imbalance\n",
    "\n",
    "We will handle the class imbalance using two approaches:\n",
    "\n",
    "1. **Oversampling the minority class:** This involves randomly duplicating examples in the minority class to increase its proportion.\n",
    "2. **Undersampling the majority class:** This involves randomly deleting examples in the majority class to decrease its proportion.\n",
    "\n",
    "As we're doing cross-validation, we need to apply over/undersampling in each fold separately (using imblearn pipelines) to avoid overly optimistic cross-validation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f81c63d",
   "metadata": {},
   "source": [
    "## Model Training with Oversampling and Undersampling\n",
    "\n",
    "Let's train Logistic Regression, Decision Tree, and Random Forest models with both oversampling and undersampling. We will use GridSearchCV to find the best hyperparameters for these models. After training, we will evaluate these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "44418c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling - Logistic Regression\n",
      "Best parameters: {}\n",
      "Best F1 score: 0.4925518701318299\n",
      "\n",
      "Oversampling - Decision Tree\n",
      "Best parameters: {'model__max_depth': 5, 'model__min_samples_split': 10}\n",
      "Best F1 score: 0.5562103679638798\n",
      "\n",
      "Oversampling - Random Forest\n",
      "Best parameters: {'model__max_depth': 15, 'model__min_samples_split': 10, 'model__n_estimators': 200}\n",
      "Best F1 score: 0.6182742823761718\n",
      "\n",
      "Undersampling - Logistic Regression\n",
      "Best parameters: {}\n",
      "Best F1 score: 0.49353512153242224\n",
      "\n",
      "Undersampling - Decision Tree\n",
      "Best parameters: {'model__max_depth': 5, 'model__min_samples_split': 5}\n",
      "Best F1 score: 0.5586581072824639\n",
      "\n",
      "Undersampling - Random Forest\n",
      "Best parameters: {'model__max_depth': 10, 'model__min_samples_split': 10, 'model__n_estimators': 200}\n",
      "Best F1 score: 0.5983632607699225\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.pipeline import Pipeline as imPipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the models and their hyperparameters\n",
    "models = [\n",
    "    {\n",
    "        'name': 'Logistic Regression',\n",
    "        'model': LogisticRegression(),\n",
    "        'params': {}\n",
    "    },\n",
    "    {\n",
    "        'name': 'Decision Tree',\n",
    "        'model': DecisionTreeClassifier(),\n",
    "        'params': {\n",
    "            'model__max_depth': [5, 10, 15, 20],\n",
    "            'model__min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'Random Forest',\n",
    "        'model': RandomForestClassifier(),\n",
    "        'params': {\n",
    "            'model__n_estimators': [100, 200, 300],\n",
    "            'model__max_depth': [5, 10, 15],\n",
    "            'model__min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# For each model, perform grid search with oversampling and print the best parameters and scores\n",
    "for model in models:\n",
    "    pipeline = imPipeline([('oversampler', RandomOverSampler()), ('scaler', StandardScaler()), ('model', model['model'])])\n",
    "    grid_search = GridSearchCV(pipeline, model['params'], cv=5, scoring='f1')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print('Oversampling -', model['name'])\n",
    "    print('Best parameters:', grid_search.best_params_)\n",
    "    print('Best F1 score:', grid_search.best_score_)\n",
    "    print()\n",
    "\n",
    "# For each model, perform grid search with undersampling and print the best parameters and scores\n",
    "for model in models:\n",
    "    pipeline = imPipeline([('undersampler', RandomUnderSampler()), ('scaler', StandardScaler()), ('model', model['model'])])\n",
    "    grid_search = GridSearchCV(pipeline, model['params'], cv=5, scoring='f1')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print('Undersampling -', model['name'])\n",
    "    print('Best parameters:', grid_search.best_params_)\n",
    "    print('Best F1 score:', grid_search.best_score_)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400250cd",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Here are the best F1 scores and parameters for each model with oversampling and undersampling:\n",
    "\n",
    "### Oversampling\n",
    "\n",
    "- Logistic Regression: F1 score is approximately 0.49. This model doesn't have any hyperparameters to tune.\n",
    "- Decision Tree: F1 score is approximately 0.565, achieved with a max depth of 5 and a min samples split of 10.\n",
    "- Random Forest: F1 score is approximately 0.618, achieved with a max depth of 15, a min samples split of 10, and 200 estimators.\n",
    "\n",
    "### Undersampling\n",
    "\n",
    "- Logistic Regression: F1 score is approximately 0.49. This model doesn't have any hyperparameters to tune.\n",
    "- Decision Tree: F1 score is approximately 0.558, achieved with a max depth of 5 and a min samples split of 5.\n",
    "- Random Forest: F1 score is approximately 0.598, achieved with a max depth of 10, a min samples split of 10, and 200 estimators.\n",
    "\n",
    "Next, let's evaluate the performance of these models on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3acdd106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling - Logistic Regression\n",
      "F1 score: 0.49601417183348095\n",
      "AUC-ROC score: 0.7143548185340536\n",
      "\n",
      "Oversampling - Decision Tree\n",
      "F1 score: 0.5595134665508253\n",
      "AUC-ROC score: 0.7740127083956799\n",
      "\n",
      "Oversampling - Random Forest\n",
      "F1 score: 0.6219974715549936\n",
      "AUC-ROC score: 0.7656840065172883\n",
      "\n",
      "Undersampling - Logistic Regression\n",
      "F1 score: 0.5044247787610618\n",
      "AUC-ROC score: 0.7219606967608316\n",
      "\n",
      "Undersampling - Decision Tree\n",
      "F1 score: 0.5572232645403377\n",
      "AUC-ROC score: 0.7608744186930272\n",
      "\n",
      "Undersampling - Random Forest\n",
      "F1 score: 0.5951219512195123\n",
      "AUC-ROC score: 0.7862983353680066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "# Define the models with the best hyperparameters\n",
    "best_models = [\n",
    "    {\n",
    "        'name': 'Logistic Regression',\n",
    "        'model': LogisticRegression(),\n",
    "        'oversampler': RandomOverSampler(),\n",
    "        'undersampler': RandomUnderSampler()\n",
    "    },\n",
    "    {\n",
    "        'name': 'Decision Tree',\n",
    "        'model': DecisionTreeClassifier(max_depth=5, min_samples_split=10),\n",
    "        'oversampler': RandomOverSampler(),\n",
    "        'undersampler': RandomUnderSampler()\n",
    "    },\n",
    "    {\n",
    "        'name': 'Random Forest',\n",
    "        'model': RandomForestClassifier(max_depth=15, min_samples_split=10, n_estimators=200),\n",
    "        'oversampler': RandomOverSampler(),\n",
    "        'undersampler': RandomUnderSampler()\n",
    "    }\n",
    "]\n",
    "\n",
    "# For each model, perform evaluation with oversampling and print the F1 score and AUC-ROC score\n",
    "for model in best_models:\n",
    "    pipeline = imPipeline([('oversampler', model['oversampler']), ('scaler', StandardScaler()), ('model', model['model'])])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    print('Oversampling -', model['name'])\n",
    "    print('F1 score:', f1_score(y_test, y_pred))\n",
    "    print('AUC-ROC score:', roc_auc_score(y_test, y_pred))\n",
    "    print()\n",
    "\n",
    "# For each model, perform evaluation with undersampling and print the F1 score and AUC-ROC score\n",
    "for model in best_models:\n",
    "    pipeline = imPipeline([('undersampler', model['undersampler']), ('scaler', StandardScaler()), ('model', model['model'])])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    print('Undersampling -', model['name'])\n",
    "    print('F1 score:', f1_score(y_test, y_pred))\n",
    "    print('AUC-ROC score:', roc_auc_score(y_test, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe18520c",
   "metadata": {},
   "source": [
    "## Model Evaluation Results\n",
    "\n",
    "Here are the F1 scores and AUC-ROC scores for each model with oversampling and undersampling on the test set:\n",
    "\n",
    "### Oversampling\n",
    "\n",
    "- Logistic Regression: F1 score is approximately 0.5 and AUC-ROC score is approximately 0.71.\n",
    "- Decision Tree: F1 score is approximately 0.56 and AUC-ROC score is approximately 0.77.\n",
    "- Random Forest: F1 score is approximately 0.62 and AUC-ROC score is approximately 0.77.\n",
    "\n",
    "### Undersampling\n",
    "\n",
    "- Logistic Regression: F1 score is approximately 0.50 and AUC-ROC score is approximately 0.72.\n",
    "- Decision Tree: F1 score is approximately 0.56 and AUC-ROC score is approximately 0.76.\n",
    "- Random Forest: F1 score is approximately 0.60 and AUC-ROC score is approximately 0.79.\n",
    "\n",
    "\n",
    "The AUC-ROC scores are higher than the F1 scores for all models. This is because the AUC-ROC score measures the ability of the model to distinguish between the positive and negative classes, regardless of the threshold used to classify the predictions. On the other hand, the F1 score depends on the specific threshold used, and is more sensitive to the imbalance of the classes.\n",
    "\n",
    "From these results, we can see that the Random Forest model with oversampling and undersampling performs the best in terms of both F1 score and AUC-ROC score. Therefore, we can conclude that handling class imbalance significantly improves the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8ddfcb",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this project, we aimed to predict whether a customer will leave the bank soon using the data on clients’ past behavior and termination of contracts with the bank. The goal was to build a model with the maximum possible F1 score, with a target of at least 0.59, and to measure the AUC-ROC metric and compare it with the F1.\n",
    "\n",
    "We started by preparing the data, which included inspecting the data, handling missing values, and encoding categorical variables. We then examined the balance of classes and found that there was a significant imbalance in the classes. We initially trained the model without taking into account the imbalance, and then improved the quality of the model by handling class imbalance using two approaches: oversampling and undersampling.\n",
    "\n",
    "We trained Logistic Regression, Decision Tree, and Random Forest models with both oversampling and undersampling, and evaluated these models using both F1 score and AUC-ROC score. The Random Forest model with oversampling and undersampling performed the best, achieving an F1 score of approximately 0.62 and an AUC-ROC score of approximately 0.77.\n",
    "\n",
    "### Project Result\n",
    "\n",
    "The project was successful in achieving its goal. The best model (Random Forest with oversampling and undersampling) exceeded the target F1 score of 0.59, indicating that it has a good balance of precision and recall and is capable of distinguishing between customers who will leave the bank soon and those who will not. The AUC-ROC score of approximately 0.77 also indicates that the model has a high true positive rate and a low false positive rate.\n",
    "\n",
    "### Business Aspects\n",
    "\n",
    "From a business perspective, this model can be very useful for the bank. By predicting whether a customer will leave the bank soon, the bank can take proactive measures to retain the customer. This could include offering special promotions or discounts, improving customer service, or addressing any issues or concerns that the customer may have. Since it is often cheaper to retain existing customers than to attract new ones, this model can help the bank to reduce costs and increase customer satisfaction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
